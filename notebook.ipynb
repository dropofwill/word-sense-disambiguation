{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching package metadata .............\n",
      "Solving package specifications: .\n",
      "\n",
      "# All requested packages already installed.\n",
      "# packages in environment at /srv/conda:\n",
      "#\n",
      "nltk                      3.2.5                      py_0    conda-forge\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} nltk\n",
    "import argparse\n",
    "import pprint\n",
    "import string\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "from nltk.probability import ConditionalProbDist\n",
    "from nltk.probability import LaplaceProbDist\n",
    "from nltk.probability import WittenBellProbDist\n",
    "from nltk.probability import LidstoneProbDist\n",
    "from nltk.probability import UniformProbDist\n",
    "from nltk import tag\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.tokenize.punkt import PunktWordTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionListClf(object):\n",
    "    \"\"\"\n",
    "    Implements the Supervised Yarowsky Decision list for the homograph\n",
    "    disambiguation problem.\n",
    "    Constructor takes as input a string of test data of the form:\n",
    "    *word:  context of word to train the classifier on\n",
    "    word:   context of the word with a different sense\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_data, test_data = None):\n",
    "        # root word without and with the prepended *\n",
    "        self.root = None        # + likelihood denotes this sense\n",
    "        self.root_star = None   # - likelihood denotes this sense\n",
    "        # The train split into a two part list [sense, [context, tokens]]\n",
    "        # It handles basic normalization by removing English stop words,\n",
    "        # punctuation, and XML tags\n",
    "        self.train = self.process_corpus(train_data)\n",
    "\n",
    "        #portion = int(len(self.train) * 0.5)\n",
    "        #self.train = self.train[:portion]\n",
    "        #print(portion)\n",
    "        #print(len(self.train))\n",
    "\n",
    "        if test_data:\n",
    "            self.test = self.process_corpus(test_data)\n",
    "        else:\n",
    "            self.test = None\n",
    "\n",
    "        # Uses NLTK's ConditionalFreqDist to count up frequencies\n",
    "        self.cfd = None\n",
    "        # Uses NLTK's ConditionalProbDist and one of the ProbDistI classes to\n",
    "        # calculate probabilities and smooth counts respectively\n",
    "        self.cpd = None\n",
    "        # Creates a list of rules sorted by their log likelihood\n",
    "        self.decision_list = []\n",
    "        # Generates the above from the train data\n",
    "        self.fit()\n",
    "\n",
    "        self.res = dict()\n",
    "        #self.prior_probability = None\n",
    "        #self.majority_label = None\n",
    "\n",
    "    def fit(self):\n",
    "        # creates self.cfd and self.cpd\n",
    "        self.generate_distributions()\n",
    "        self.generate_decision_list()\n",
    "\n",
    "    def generate_distributions(self, smooth=None):\n",
    "        \"\"\"\n",
    "        Creates the conditional frequency and probability distributions to\n",
    "        generate the decision list rules.\n",
    "        \"\"\"\n",
    "        self.cfd = ConditionalFreqDist()\n",
    "\n",
    "        self.k_word_dist(self.train, 1)\n",
    "        self.k_word_dist(self.train, -1)\n",
    "        self.k_window_dist(self.train, 5)\n",
    "        self.k_tag_dist(self.train, 1)\n",
    "        self.k_tag_dist(self.train, -1)\n",
    "\n",
    "        if smooth:\n",
    "            self.cpd = ConditionalProbDist(self.cfd, smooth)\n",
    "        else:\n",
    "            #self.cpd = ConditionalProbDist(self.cfd, WittenBellProbDist, 3)\n",
    "            self.cpd = ConditionalProbDist(self.cfd, LidstoneProbDist, 0.1)\n",
    "            #self.cpd = ConditionalProbDist(cfd, LaplaceProbDist)\n",
    "            #self.cpd = ConditionalProbDist(cfd, UniformProbDist)\n",
    "\n",
    "    def generate_decision_list(self):\n",
    "        for rule in self.cpd.conditions():\n",
    "            likelihood = self.calculate_log_likelihood(rule)\n",
    "            self.decision_list.append([rule, likelihood])\n",
    "        # instead of always applying the abs, I opted to apply only while\n",
    "        # sorting, as the sign is an easy way to denote sense:\n",
    "        # + for root / - for root star\n",
    "        self.decision_list.sort(key=lambda rule: math.fabs(rule[1]), reverse=True)\n",
    "        #pp.pprint(self.decision_list)\n",
    "\n",
    "    def evaluate(self, test_data=None):\n",
    "        if test_data:\n",
    "            self.test = self.process_corpus(test_data)\n",
    "\n",
    "        root_prior, root_star_prior = 0.0, 0.0\n",
    "        for line in self.train:\n",
    "            if line[0] == self.root:\n",
    "                root_prior += 1.0\n",
    "            elif line[0] == self.root_star:\n",
    "                root_star_prior += 1.0\n",
    "            else:\n",
    "                print(\"warning no match\")\n",
    "\n",
    "        self.res[\"total\"] = root_star_prior + root_prior\n",
    "        self.res[\"root_prior\"] = root_prior / self.res[\"total\"]\n",
    "        self.res[\"root_star_prior\"] = root_star_prior / self.res[\"total\"]\n",
    "\n",
    "        if self.res[\"root_star_prior\"] > self.res[\"root_prior\"]:\n",
    "            self.majority_label = self.root_star\n",
    "            self.res[\"prior_probability\"] = self.res[\"root_star_prior\"]\n",
    "        else:\n",
    "            self.majority_label = self.root\n",
    "            self.res[\"prior_probability\"] = self.res[\"root_prior\"]\n",
    "\n",
    "        predictions, references = [], []\n",
    "        self.res[\"correct\"], self.res[\"incorrect\"], self.res[\"all_res\"] = [], [], []\n",
    "        for context in self.test:\n",
    "            pred, ref, r, c = self.predict(context)\n",
    "            predictions.append(pred)\n",
    "            references.append(ref)\n",
    "            if pred == ref:\n",
    "                self.res[\"correct\"].append([pred, ref, r, c])\n",
    "                self.res[\"all_res\"].append([r[1], \"correct\"])\n",
    "            elif pred != ref:\n",
    "                self.res[\"incorrect\"].append([pred, ref, r, c])\n",
    "                self.res[\"all_res\"].append([r[1], \"incorrect\"])\n",
    "\n",
    "        pp.pprint(self.res[\"all_res\"])\n",
    "        #pp.pprint(self.res[\"correct\"])\n",
    "        #pp.pprint(self.res[\"incorrect\"])\n",
    "\n",
    "        self.res[\"predictions\"] = predictions\n",
    "        self.res[\"references\"] = references\n",
    "        self.res[\"cm\"] = self.confustion_matrix(predictions, references)\n",
    "\n",
    "        self.res[\"accuracy\"] = self.accuracy(predictions, references)\n",
    "        self.res[\"error\"] = 1.0 - self.res[\"accuracy\"]\n",
    "        self.res[\"baseline_error\"] = 1.0 - self.res[\"prior_probability\"]\n",
    "        self.res[\"error_reduction\"] = \\\n",
    "            self.error_reduction(self.res[\"error\"], self.res[\"baseline_error\"])\n",
    "\n",
    "        self.res[\"root_star_precision\"], self.res[\"root_precision\"] = \\\n",
    "                self.precisions(self.res[\"cm\"])\n",
    "        self.res[\"root_star_recall\"], self.res[\"root_recall\"] = \\\n",
    "                self.recalls(self.res[\"cm\"])\n",
    "\n",
    "        self.res[\"macro_precision\"], self.res[\"macro_recall\"] = self.macro_average( \\\n",
    "                                            self.res[\"root_star_precision\"],\\\n",
    "                                            self.res[\"root_precision\"], \\\n",
    "                                            self.res[\"root_star_recall\"], \\\n",
    "                                            self.res[\"root_recall\"])\n",
    "        # bin log-likelihood by casting as an int\n",
    "        self.res[\"dlist_dist\"] = [int(r[1]) for r in self.decision_list]\n",
    "        #self.res[\"dlist_dist\"] = [math.fabs(int(r[1])) for r in self.decision_list]\n",
    "        #pp.pprint(self.res[\"dlist_dist\"])\n",
    "\n",
    "        self.res[\"cnd_dist\"] = ConditionalFreqDist()\n",
    "        for res in self.res[\"all_res\"]:\n",
    "            #condition = str(res[0]) + \"_\" + str(res[1])\n",
    "            self.res[\"cnd_dist\"][str(math.fabs(int(res[0])))][str(res[1])] += 1\n",
    "\n",
    "        self.res[\"cnd_dist\"].tabulate()\n",
    "\n",
    "    def print_results(self):\n",
    "        print\n",
    "        print(\"int-binned log-likelihood distributions:\")\n",
    "        ll_fdist = FreqDist(self.res[\"dlist_dist\"])\n",
    "        ll_fdist.tabulate()\n",
    "        print\n",
    "        print(self.res[\"cm\"])\n",
    "\n",
    "        print(\"{:<30}{:>.3%}\"\n",
    "                .format(\"Majority Class Prior Prob: \",\n",
    "                   self.res[\"prior_probability\"]))\n",
    "        print(\"{:<30}{:>}\"\n",
    "                .format(\"Majority Class Label: \", self.majority_label))\n",
    "\n",
    "        print\n",
    "        print(\"{:<30}{:>.3%}\"\n",
    "                .format(\"Accuracy: \", self.res[\"accuracy\"]))\n",
    "        print(\"{:<30}{:>.3%}\"\n",
    "                .format(\"Error: \", self.res[\"error\"]))\n",
    "        print(\"{:<30}{:>.3%}\"\n",
    "                .format(\"Error Reduction / Baseline: \",\n",
    "                    self.res[\"error_reduction\"]))\n",
    "\n",
    "        print\n",
    "        print(\"{:<7}{:<23}{:>.3%}\"\n",
    "                .format(self.root_star,\n",
    "                    \"Precision: \",\n",
    "                    self.res[\"root_star_precision\"]))\n",
    "        print(\"{:<7}{:<23}{:>.3%}\"\n",
    "                .format(self.root,\n",
    "                    \"Precision: \",\n",
    "                    self.res[\"root_precision\"]))\n",
    "        print(\"{:<7}{:<23}{:>.3%}\"\n",
    "                .format(self.root_star,\n",
    "                    \"Recall: \",\n",
    "                    self.res[\"root_star_recall\"]))\n",
    "        print(\"{:<7}{:<23}{:>.3%}\"\n",
    "                .format(self.root,\n",
    "                    \"Recall: \",\n",
    "                    self.res[\"root_recall\"]))\n",
    "\n",
    "        print\n",
    "        print(\"{:<30}{:>.3%}\"\n",
    "                .format(\"Macro Precision: \", self.res[\"macro_precision\"]))\n",
    "        print(\"{:<30}{:>.3%}\"\n",
    "                .format(\"Macro Recall: \", self.res[\"macro_recall\"]))\n",
    "        print\n",
    "        print(\"Top Ten Rules:\")\n",
    "        for l in self.decision_list[:10]:\n",
    "            print(\"{:<30}{:>.4}\".format(l[0], l[1]))\n",
    "        print\n",
    "        print(\"3 Correct:\")\n",
    "        for l in self.res[\"correct\"][:3]:\n",
    "            print(\"Correctly Predicted: {} \\n Rule: {}, log-likelihood: {} \\n {}\"\n",
    "                    .format(l[0], l[2][0], l[2][1], \" \".join(l[3])))\n",
    "        print\n",
    "        print(\"3 Incorrect:\")\n",
    "        for l in self.res[\"incorrect\"][:3]:\n",
    "            print(\"Predicted: {}, was actually: {} \\n Rule: {}, log-likelihood: {} \\n {}\"\n",
    "                    .format(l[0], l[1], l[2][0], l[2][1], \" \".join(l[3])))\n",
    "\n",
    "    def confustion_matrix(self, predictions, references):\n",
    "        return ConfusionMatrix(references, predictions)\n",
    "\n",
    "    def accuracy(self, predictions, references):\n",
    "        correct, total = 0.0, 0.0\n",
    "        for i, p in enumerate(predictions):\n",
    "            if p == references[i]:\n",
    "                correct += 1.0\n",
    "            total += 1.0\n",
    "        return correct / total\n",
    "\n",
    "    def error_reduction(self, my_error, base_error):\n",
    "        # 1 - (my error/baseline error)\n",
    "        return 1.0 - (float(my_error) / float(base_error))\n",
    "\n",
    "    def precisions(self, cm):\n",
    "        confusions = cm._confusion\n",
    "        r_star_tp = float(confusions[0][0])\n",
    "        r_star_fp = float(confusions[0][1])\n",
    "        r_star_fn = float(confusions[1][0])\n",
    "\n",
    "        r_tp = float(confusions[1][1])\n",
    "        r_fp = float(confusions[1][0])\n",
    "        r_fn = float(confusions[0][1])\n",
    "\n",
    "        if r_star_tp + r_star_fp == 0:\n",
    "            r_star_precision = 0\n",
    "        else:\n",
    "            r_star_precision = r_star_tp / (r_star_tp + r_star_fp)\n",
    "\n",
    "        if r_tp + r_fp == 0:\n",
    "            r_precision = 0\n",
    "        else:\n",
    "            r_precision = r_tp / (r_tp + r_fp)\n",
    "\n",
    "        return (r_star_precision, r_precision)\n",
    "\n",
    "    def recalls(self, cm):\n",
    "        confusions = cm._confusion\n",
    "        r_star_tp = float(confusions[0][0])\n",
    "        r_star_fp = float(confusions[0][1])\n",
    "        r_star_fn = float(confusions[1][0])\n",
    "\n",
    "        r_tp = float(confusions[1][1])\n",
    "        r_fp = float(confusions[1][0])\n",
    "        r_fn = float(confusions[0][1])\n",
    "\n",
    "        if r_star_tp + r_star_fn == 0:\n",
    "            r_star_precision = 0\n",
    "        else:\n",
    "            r_star_precision = r_star_tp / (r_star_tp + r_star_fn)\n",
    "\n",
    "        if r_tp + r_fn == 0:\n",
    "            r_precision = 0\n",
    "        else:\n",
    "            r_precision = r_tp / (r_tp + r_fn)\n",
    "\n",
    "        return (r_star_precision, r_precision)\n",
    "\n",
    "    def macro_average(self, p1, p2, r1, r2):\n",
    "        macro_precision = (float(p1) + float(p2)) / 2.0\n",
    "        macro_recall = (float(r1) + float(r2)) / 2.0\n",
    "        return (macro_precision, macro_recall)\n",
    "\n",
    "    def predict(self, context):\n",
    "        \"\"\"\n",
    "        Predict with ground truth in the same form as the train data\n",
    "        Returns tuple of the form (prediction, actual, rule, context)\n",
    "        \"\"\"\n",
    "        if type(context) != list:\n",
    "            context = self.process_corpus(context)\n",
    "\n",
    "        for rule in self.decision_list:\n",
    "            if self.check_rule(context[1], context[2], rule[0]):\n",
    "                # + implies root, - implies root_star\n",
    "                #print(rule)\n",
    "                if rule[1] > 0:\n",
    "                    return (self.root, context[0], rule, context[1])\n",
    "                elif rule[1] < 0:\n",
    "                    return (self.root_star, context[0], rule, context[1])\n",
    "\n",
    "        #print(None)\n",
    "        # Default to majority label\n",
    "        return (self.majority_label, context[0], \"default\", context[1])\n",
    "\n",
    "    def check_rule(self, context, tags, rule):\n",
    "        \"\"\"\n",
    "        Given a rule and a context\n",
    "        Returns whether the rule applies to the context\n",
    "        \"\"\"\n",
    "        rule_scope, rule_type, rule_feature = rule.split(\"_\")\n",
    "        rule_scope = int(rule_scope)\n",
    "        #print(rule_scope, rule_type, rule_feature)\n",
    "\n",
    "        if rule_type == \"word\":\n",
    "            return self.check_k_word(rule_scope, context, rule_feature)\n",
    "        elif rule_type == \"window\":\n",
    "            return self.check_k_window(rule_scope, context, rule_feature)\n",
    "        elif rule_type == \"tag\":\n",
    "            return self.check_k_tag(rule_scope, context, tags, rule_feature)\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def k_word_dist(self, corpus, k):\n",
    "        for line in corpus:\n",
    "            sense, context = line[0], line[1]\n",
    "            k_word = self.get_k_word(k, context)\n",
    "            if k_word:\n",
    "                # create freqdist for each sense per word\n",
    "                condition = str(k) + \"_word_\" + re.sub(r'\\_', '', k_word)\n",
    "                self.cfd[condition][sense] += 1\n",
    "\n",
    "    def k_tag_dist(self, corpus, k):\n",
    "        for line in corpus:\n",
    "            sense, context, tags = line[0], line[1], line[2]\n",
    "            k_tag = self.get_k_tag(k, context, tags)\n",
    "            if k_tag:\n",
    "                # create freqdist for each sense per word\n",
    "                condition = str(k) + \"_tag_\" + k_tag\n",
    "                self.cfd[condition][sense] += 1\n",
    "\n",
    "    def k_window_dist(self, corpus, k):\n",
    "        for line in corpus:\n",
    "            k_ = k\n",
    "            sense, context = line[0], line[1]\n",
    "            while k_ > 0:\n",
    "                pos_k_word = self.get_k_word(k_, context)\n",
    "                neg_k_word = self.get_k_word(-1 * k_, context)\n",
    "                if pos_k_word:\n",
    "                    condition = str(k)+\"_window_\"+re.sub(r'\\_', '', pos_k_word)\n",
    "                    self.cfd[condition][sense] += 1\n",
    "                if neg_k_word:\n",
    "                    condition = str(k)+\"_window_\"+re.sub(r'\\_', '', neg_k_word)\n",
    "                    self.cfd[condition][sense] += 1\n",
    "                k_ -= 1\n",
    "\n",
    "    def check_k_word(self, k, context, check_word):\n",
    "        return self.get_k_word(k, context) == check_word\n",
    "\n",
    "    def check_k_window(self, k, context, check_word):\n",
    "        k_ = k\n",
    "        while k_ > 0:\n",
    "            if self.get_k_word(k_, context) == check_word:\n",
    "                return True\n",
    "            if self.get_k_word(-1 * k_, context) == check_word:\n",
    "                return True\n",
    "            k_ -= 1\n",
    "        return False\n",
    "\n",
    "    def check_k_tag(self, k, context, tags, check_tag):\n",
    "        return self.get_k_tag(k, context, tags) == check_tag\n",
    "\n",
    "    def get_k_word(self, k, context):\n",
    "        root_i = context.index(self.root)\n",
    "        k_word_i = root_i + k\n",
    "        if len(context) > k_word_i and k_word_i >= 0:\n",
    "            return context[k_word_i]\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_k_tag(self, k, context, tags):\n",
    "        root_i = context.index(self.root)\n",
    "        k_tag_i = root_i + k\n",
    "        if len(tags) > k_tag_i and k_tag_i >= 0:\n",
    "            return tags[k_tag_i]\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def calculate_log_likelihood(self, rule):\n",
    "        prob = self.cpd[rule].prob(self.root)\n",
    "        prob_star = self.cpd[rule].prob(self.root_star)\n",
    "        div = prob / prob_star\n",
    "        # -means prob_star, +means prob\n",
    "        if div == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return math.log(div, 2)\n",
    "\n",
    "    def process_corpus(self, text):\n",
    "        \"\"\"\n",
    "        Process an input of the form:\n",
    "        *word:  context of word to train the classifier on\n",
    "        word:   context of the word with a different sense\n",
    "        \"\"\"\n",
    "        # split the text into its individual senses and contexts\n",
    "        corpus = text.split(\"\\n\")\n",
    "        # split the sense from the context\n",
    "        corpus = [l.split(\"\\t\") for l in corpus if l != '']\n",
    "        # strip the colon from the sense\n",
    "        corpus = [[l[0][:-1], l[1]] for l in corpus]\n",
    "        # remove XML tags from corpus\n",
    "        corpus = [[l[0], re.sub(r'\\<.*?(\\>|$)', '', l[1])] for l in corpus]\n",
    "        # Punkt tokenize the context\n",
    "        corpus = [[l[0], tokenizer.tokenize(l[1].lower())] for l in corpus]\n",
    "        # Get rid of stop words and punctuation from the context\n",
    "        stop_words = stopwords.words(\"english\")\n",
    "        stop_words.extend(string.punctuation)\n",
    "        # Get pos tags, but store them in adjacent array because it makes\n",
    "        # root sense lookup easier\n",
    "        corpus = [[l[0], [w for w in tag.pos_tag(l[1])]] for l in corpus]\n",
    "        # Remove punctuation from words\n",
    "        corpus = [[l[0], [(re.sub(r'[\\.\\,\\?\\!\\'\\\"\\-\\_]','', w[0]), w[1]) for w in l[1]]] for l in corpus]\n",
    "        # only keep context words that aren't in our stop words list and that\n",
    "        # are shorter than two characters long\n",
    "        corpus = [[l[0], [w for w in l[1] if w[0] not in stop_words and len(w[0]) > 1]] for l in corpus]\n",
    "        # get root word without the * by looking at the first example\n",
    "        self.root = re.sub(r'\\*', '', corpus[0][0])\n",
    "        self.root_star = \"*\" + self.root\n",
    "        # Change the structure of the corpus\n",
    "        pos_corpus = []\n",
    "        for l in corpus:\n",
    "            temp_w, temp_t = [], []\n",
    "            for w_t in l[1]:\n",
    "                temp_w.append(w_t[0])\n",
    "                temp_t.append(w_t[1])\n",
    "            pos_corpus.append([l[0], temp_w, temp_t])\n",
    "\n",
    "        #pp.pprint(pos_corpus)\n",
    "        return pos_corpus\n",
    "\n",
    "    def test_based_on_paper_results(self):\n",
    "        # Should equal ~7.14 according to Yarowsky given test data string\n",
    "        #if self.train == test_data:\n",
    "            print(\"This likelihood should equal ~7.14 if it was feed the `test_data` string according to Yarowksy's paper\")\n",
    "            print(self.calculate_log_likelihood(\"-1_word_sea\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limited to predicting only binary senses, in this example bass the instrument and bass the fish.\n",
    "# In the data this is denoted as *bass for the fish and bass for the instrument.\n",
    "# If you can munge your data in this format you could run the code below to compare your rates (note tabs \\t denote the start of the sentence.\n",
    "\n",
    "train_data = \\\n",
    "\"\"\"\n",
    "*bass:\tportion of shrimp, mussels, sea bass and whatnot in a spicy,\n",
    "bass:\tStephan Weidner, the composer and bass player for Boehse Onkelz, a\n",
    "*bass:\trestaurant, waiters serve pecan-crusted sea bass ($18.95) and peppered rib eye\n",
    "*bass:\trestaurant, waiters serve pecan-crusted sea bass ($18.95) and peppered rib eye\n",
    "*bass:\trestaurant, waiters serve pecan-crusted sea bass ($18.95) and peppered rib eye\n",
    "*bass:\trestaurant, waiters serve pecan-crusted sea bass ($18.95) and peppered rib eye\n",
    "*bass:\trestaurant, waiters serve pecan-crusted sea bass ($18.95) and peppered rib eye\n",
    "*bass:\trestaurant, waiters serve pecan-crusted sea bass ($18.95) and peppered rib eye\n",
    "\"\"\"\n",
    "\n",
    "test_data = \\\n",
    "\"\"\"\n",
    "*bass:\t-- OPTIONAL MATERIAL FOLLOWS.) Striped bass are also being spotted in\n",
    "*bass:\tsource of entertainment is pay-per-view bass fishing. Yet this is still\n",
    "*bass:\therring and the enormous striped bass that feed on them. It\n",
    "bass:\tvalued at $250,000. Another double bass trapped in the room is\n",
    "*bass:\tand dining on Chilean sea bass and poached peaches. <DOC id=\"APW20010228.0028\"\n",
    "*bass:\t<DOC id=\"NYT20010802.0256\" type=\"story\" > Japan's bass fisherman become homeland heroes New\n",
    "*bass:\trestaurant, waiters serve pecan-crusted sea bass ($18.95) and peppered rib eye\n",
    "*bass:\trestaurant, waiters serve pecan-crusted sea bass ($18.95) and peppered rib eye\n",
    "*bass:\trestaurant, waiters serve pecan-crusted sea bass ($18.95) and peppered rib eye\n",
    "*bass:\trestaurant, waiters serve pecan-crusted sea bass ($18.95) and peppered rib eye\n",
    "*bass:\trestaurant, waiters serve pecan-crusted sea bass ($18.95) and peppered rib eye\n",
    "*bass:\trestaurant, waiters serve pecan-crusted sea bass ($18.95) and peppered rib eye\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionListClf(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   [-6.339850002884624, 'correct'],\n",
      "    [-6.339850002884624, 'incorrect'],\n",
      "    [-6.339850002884624, 'correct'],\n",
      "    [-6.339850002884624, 'correct'],\n",
      "    [-6.339850002884624, 'correct'],\n",
      "    [-6.339850002884624, 'correct'],\n",
      "    [-6.339850002884624, 'correct'],\n",
      "    [-6.339850002884624, 'correct']]\n",
      "      correct incorrect \n",
      "6.0         7         1 \n"
     ]
    }
   ],
   "source": [
    "clf.evaluate(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int-binned log-likelihood distributions:\n",
      "-3  3 -5 -4 -6 \n",
      "36 11 10  4  3 \n",
      "      | *   |\n",
      "      | b b |\n",
      "      | a a |\n",
      "      | s s |\n",
      "      | s s |\n",
      "------+-----+\n",
      "*bass |<7>. |\n",
      " bass | 1<.>|\n",
      "------+-----+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Majority Class Prior Prob:    91.667%\n",
      "Majority Class Label:         *bass\n",
      "Accuracy:                     87.500%\n",
      "Error:                        12.500%\n",
      "Error Reduction / Baseline:   -50.000%\n",
      "*bass  Precision:             100.000%\n",
      "bass   Precision:             0.000%\n",
      "*bass  Recall:                87.500%\n",
      "bass   Recall:                0.000%\n",
      "Macro Precision:              50.000%\n",
      "Macro Recall:                 43.750%\n",
      "Top Ten Rules:\n",
      "-1_tag_NN                     -6.34\n",
      "-1_word_sea                   -6.15\n",
      "5_window_sea                  -6.15\n",
      "1_word_($1895)                -5.931\n",
      "5_window_waiters              -5.931\n",
      "5_window_eye                  -5.931\n",
      "5_window_serve                -5.931\n",
      "5_window_rib                  -5.931\n",
      "5_window_pecan                -5.931\n",
      "5_window_peppered             -5.931\n",
      "3 Correct:\n",
      "Correctly Predicted: *bass \n",
      " Rule: -1_tag_NN, log-likelihood: -6.339850002884624 \n",
      " portion shrimp mussels sea bass whatnot spicy\n",
      "Correctly Predicted: *bass \n",
      " Rule: -1_tag_NN, log-likelihood: -6.339850002884624 \n",
      " restaurant waiters serve pecan crusted sea bass ($1895) peppered rib eye\n",
      "Correctly Predicted: *bass \n",
      " Rule: -1_tag_NN, log-likelihood: -6.339850002884624 \n",
      " restaurant waiters serve pecan crusted sea bass ($1895) peppered rib eye\n",
      "3 Incorrect:\n",
      "Predicted: *bass, was actually: bass \n",
      " Rule: -1_tag_NN, log-likelihood: -6.339850002884624 \n",
      " stephan weidner composer bass player boehse onkelz\n"
     ]
    }
   ],
   "source": [
    "clf.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
